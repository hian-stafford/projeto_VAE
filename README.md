# Variational Autoencoder (VAE) com PyTorch ‚Äì Gera√ß√£o de D√≠gitos Manuscritos

Este projeto implementa um **Autoencoder Variacional (VAE)** utilizando a biblioteca **PyTorch**, com foco na gera√ß√£o e reconstru√ß√£o de imagens da base **MNIST** (d√≠gitos manuscritos). √â um exemplo did√°tico e pr√°tico de aplica√ß√£o de modelos generativos baseados em aprendizado profundo.

---

## ‚ú® O que este projeto faz?

- Treina um **VAE simples** para comprimir e reconstruir imagens de d√≠gitos (0 a 9).
- Aprende uma **representa√ß√£o latente (vetores z)** para o espa√ßo de imagens.
- Permite **gerar novas imagens realistas** a partir de vetores amostrados aleatoriamente.
- Visualiza os resultados com uso de `matplotlib`.

---

## üß† Por que usar um VAE?

O VAE √© um tipo de autoencoder **probabil√≠stico**, capaz de:
- Aprender **representa√ß√µes compactas e estruturadas** de dados complexos.
- Realizar **gera√ß√£o de dados sint√©ticos**, √∫til em diversos cen√°rios.
- Possibilitar **interpola√ß√£o e explora√ß√£o do espa√ßo latente** de forma controlada.

---

## üöÄ Tecnologias e ferramentas utilizadas

| Ferramenta | Utilidade |
|-----------|-----------|
| [PyTorch](https://pytorch.org) | Framework principal de deep learning usado para criar e treinar o modelo. |
| `torchvision.datasets.MNIST` | Utilizada para carregar facilmente o conjunto de dados de treino. |
| `matplotlib` | Gera√ß√£o de gr√°ficos e imagens de sa√≠da do modelo. |
| CPU ou CUDA | Suporte para execu√ß√£o tanto em m√°quinas sem GPU quanto com GPU Nvidia. |

---

## üìö Dataset utilizado

- **MNIST**: conjunto de 60.000 imagens de treino e 10.000 de teste, com d√≠gitos manuscritos de 0 a 9.
- Dimens√µes: 28x28 pixels, escala de cinza.

---

## üí° Poss√≠veis aplica√ß√µes reais

- **Gera√ß√£o de imagens realistas** (ex: rostos, roupas, arte).
- **Compress√£o de dados com controle probabil√≠stico**.
- **An√°lise de anomalias** (comparando reconstru√ß√µes).
- **Interpola√ß√£o e transfer√™ncia de estilo** em dom√≠nios visuais.
- **Pr√©-processamento e representa√ß√£o de dados** em tarefas downstream.
    - Classificar que n√∫mero est√° na imagem (0 a 9)
    - Detectar se uma imagem √© an√¥mala ou corrompida
    - Agrupar imagens semelhantes
    - Reduzir dimensionalidade para visualiza√ß√£o
    - (Todas essas s√£o tarefas downstream, porque usam o que o VAE aprendeu como base.)

---

## üõ†Ô∏è Habilidades aplicadas

- Implementa√ß√£o de modelos generativos com **PyTorch**.
- Constru√ß√£o de arquiteturas com `nn.Module`.
- Aplica√ß√£o de **fun√ß√£o de perda com KL divergence**.
- Uso de t√©cnicas de **reparametriza√ß√£o** para treinamento com backpropagation.
- Manipula√ß√£o de imagens com `torchvision`.
- Gera√ß√£o e visualiza√ß√£o de dados sint√©ticos.

---

## üì∑ Exemplos de sa√≠da

Abaixo, uma amostra de imagens **geradas pelo modelo VAE** ap√≥s o treinamento com MNIST:

![output](https://github.com/user-attachments/assets/0ec4d76b-fd32-4ccc-b3e0-cc496051c3c1)

---

## üë§ Autor

**Hian Stafford**  
üìß [hian.correa@gmail.com](mailto:hian.correa@gmail.com)

---
